colabì—ì„œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•

(0)
ë¨¼ì € ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜•ì„ gpuë¡œ

(1)
!git clone https://github.com/diyung0/nlp.git

(2)
import os
import subprocess

%cd nlp
print("í˜„ì¬ ë””ë ‰í† ë¦¬:", os.getcwd())

(3)
!pip install torch torchvision torchaudio transformers PyPDF2 langchain langchain-text-splitters langchain-experimental faiss-cpu langchain-community bert-score pandas langchain-ollama

(4)
import subprocess
import time
import requests

# Ollama ì„¤ì¹˜
!curl -fsSL https://ollama.com/install.sh | sh

# Ollama ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘
def start_ollama():
    process = subprocess.Popen(['ollama', 'serve'], 
                              stdout=subprocess.PIPE, 
                              stderr=subprocess.PIPE)
    print("ğŸš€ Ollama ì„œë²„ë¥¼ ì‹œì‘í•˜ëŠ” ì¤‘...")
    time.sleep(15)  # ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸°
    
    # ì„œë²„ ìƒíƒœ í™•ì¸
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=10)
        print("âœ… Ollama ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return True
    except Exception as e:
        print(f"âš ï¸ ì„œë²„ í™•ì¸ ì¤‘: {e}")
        return False

start_ollama()


(5)
!ollama pull bge-m3

(6)
# ë°ì´í„° ë””ë ‰í† ë¦¬ì™€ PDF íŒŒì¼ í™•ì¸
print("ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸:")
!ls -la data/

# PDF íŒŒì¼ì´ ì—†ë‹¤ë©´ ì—…ë¡œë“œ
if not os.path.exists('data/test2.pdf'):
    print("âš ï¸ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    os.makedirs('data', exist_ok=True)
else:
    print("âœ… PDF íŒŒì¼ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


(7)
# í™˜ê²½ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
def quick_test():
    try:
        import torch
        import transformers
        import langchain
        from langchain_ollama import OllamaEmbeddings
        
        print("âœ… PyTorch:", torch.__version__)
        print("âœ… Transformers:", transformers.__version__)
        print("âœ… LangChain ì„í¬íŠ¸ ì„±ê³µ")
        print("âœ… Ollama ì„ë² ë”© ì„í¬íŠ¸ ì„±ê³µ")
        
        # GPU í™•ì¸
        if torch.cuda.is_available():
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
            
        return True
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

quick_test()


(8)
!python final.py --pdf_path data/test_real.pdf --chunk_method recursive

