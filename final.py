import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn.functional as F
from torch import nn
from transformers import GPT2Tokenizer, GPT2LMHeadModel

import argparse
from PyPDF2 import PdfReader
from langchain.schema import Document
from langchain_text_splitters  import CharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS


import pandas as pd
import time
import json

class QAGenerator(nn.Module):
    def __init__(self, model_name='gpt2-large'):
        super().__init__()
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits
    
    def get_device(self):
        return next(self.model.parameters()).device
    
    @torch.no_grad()
    def generate_answer(self, context, question, max_length=50, temperature=0.1, top_p=0.9):
        """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± (Few-shot í”„ë¡¬í”„íŠ¸ í¬í•¨)"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""You must answer using ONLY the information in the context below and respond with either YES or NO.
        Context: {context}
        Question: {question}
        Answer: """

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=1024, truncation=True)
        inputs = inputs.to(self.get_device())
        
        # ìƒì„±
        for _ in range(max_length):
            # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
            logits = self.forward(inputs)
            next_token_logits = logits[0, -1, :] / temperature
            
            # Top-p ìƒ˜í”Œë§
            probs = F.softmax(next_token_logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
            
            # Top-p ë§ˆìŠ¤í¬ ìƒì„±
            mask = cumsum_probs <= top_p
            mask[1:] = mask[:-1].clone()
            mask[0] = True
            
            # í•„í„°ë§ëœ í™•ë¥ ë¡œ ìƒ˜í”Œë§
            filtered_probs = sorted_probs * mask.float()
            filtered_probs = filtered_probs / filtered_probs.sum()
            
            # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            next_token_idx = torch.multinomial(filtered_probs, 1)
            next_token = sorted_indices[next_token_idx]
            
            # ì¢…ë£Œ ì¡°ê±´ë“¤
            token_text = self.tokenizer.decode(next_token.item())
            if (next_token.item() == self.tokenizer.eos_token_id or 
                token_text in ['\n\nContext:', '\nContext:', 'Question:'] or
                '\n\n' in token_text):
                break
                
            # ìƒˆ í† í°ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
            inputs = torch.cat([inputs, next_token.unsqueeze(0)], dim=-1)
        
        # ë””ì½”ë”©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        generated_text = self.tokenizer.decode(inputs[0], skip_special_tokens=True)

        # print(f"\n\n\nGenerated text: {generated_text}\n\n\n")

        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆì§€ë§‰ Answer: ì´í›„ ë¶€ë¶„)
        if "Answer:" in generated_text:
            answer_parts = generated_text.split("Answer:")
            answer = answer_parts[-1].strip()
        else:
            answer = "Could not generate an answer."

        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        if '\n' in answer:
            answer = answer.split('\n')[0].strip()

        return answer


def pdf_to_text(pdf_path):
    """Convert PDF to text."""
    reader = PdfReader(pdf_path)
    pages = [page.extract_text() for page in reader.pages]
    return "\n".join(pages)


def chunk_text(text, chunk_method, embeddings_model=None):
    """Chunk text using the specified method."""
    if chunk_method == 'character':
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=1500,
            chunk_overlap=100,
            length_function=len,
            is_separator_regex=False
        )
    elif chunk_method == 'recursive':
        text_splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", " ", ""],
            chunk_size=1500,
            chunk_overlap=100,
            length_function=len,
            is_separator_regex=False
        )
    elif chunk_method == 'semantic':
        if embeddings_model:
            text_splitter = SemanticChunker(embeddings_model)
        else:
            raise ValueError("Embeddings model must be provided for semantic chunking")
    else:
        raise ValueError("Unsupported chunk method")

    documents = [Document(page_content=text, metadata={"page_number": 1})]
    return text_splitter.split_documents(documents)


def save_to_file(data, file_path):
    """Save data to a file."""
    with open(file_path, 'w', encoding='utf-8') as f:
        if isinstance(data, (dict, list)):
            json.dump(data, f, ensure_ascii=False, indent=4)
        else:
            f.write(data)


def main(args):
    """ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ëª¨ë¸ ì´ˆê¸°í™”
    qa_generator = QAGenerator()
    qa_generator.to(device)
    qa_generator.eval()

    # PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # print(f"Reading PDF file: {args.pdf_path}")
    # text = pdf_to_text(args.pdf_path)
    # print("PDF text extraction complete.")
    # print(f"Extracted text length: {len(text)} characters")

    # # í…ìŠ¤íŠ¸ ì €ì¥
    # save_to_file(text, "extracted_text.txt")
    # print("ğŸ“„ Extracted text saved to 'extracted_text.txt'")

    # í…ìŠ¤íŠ¸ ì½ê¸°
    with open("extracted_text.txt", "r", encoding="utf-8") as f:
        text = f.read()

    # ì²­í‚¹ ì „ëµì„ ì¸ìë¡œ ë°›ì•„ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
    print(f"Chunking text using method: {args.chunk_method}")
    documents = chunk_text(
        text,
        chunk_method=args.chunk_method,
        embeddings_model=OllamaEmbeddings(model="bge-m3") if args.chunk_method == 'semantic' else None
    )
    print(f"Number of chunks created: {len(documents)}")

    # ì²­í¬ ì €ì¥
    chunks_data = [doc.page_content for doc in documents]
    save_to_file(chunks_data, "chunks.json")
    print("ğŸ“„ Chunks saved to 'chunks.json'")

    # (4) ì²­í¬ë¥¼ ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ vectorí™”
    embeddings_model = OllamaEmbeddings(model="bge-m3")

    vector_store = FAISS.from_documents(
        documents=documents,
        embedding=embeddings_model
    )
    print("Vector store created with FAISS.")

    # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
    vector_store.save_local("vector_store")
    print("ğŸ“„ Vector store saved to 'vector_store' directory")

    # (5) ì§ˆë¬¸ ë°ì´í„°ì…‹ì„ ì½ì–´ì™€ì„œ vectorí™”ëœ ì²­í¬ì™€ ì§ˆë¬¸ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ì°¾ê¸°
    # (6) ê´€ë ¨ ì²­í¬(context)ì™€ ì§ˆë¬¸, few-shot ì˜ˆì œë¥¼ gpt2ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹µë³€ ìƒì„±

    questions = [
        {
            # Q1 â­ï¸
            "question": "Can the duration of study be reduced by up to one year in the Integrated Program?"
            # YES
        },
        {
            # Q2 â­ï¸ â­ï¸
            "question": "Is the Undergraduate-Graduate Integrated Program designed to reduce the duration of study?"
            # YES
        },
        {
            # Q3
            "question": "Is the Integrated Program designed to reduce the duration of study?"
        },
        {
            # Q4 â­ï¸ âŒ
            "question": "Do applicants need to submit specific documents to apply for the Integrated Program?"
        },
        {
            # Q5 â­ï¸
            "question": "Is the selection process for the Integrated Program based on document screening?"
        },
        {
            # Q6
            "question": "Can students apply for the Integrated Program after completing 4 semesters"
        },
        {
            # Q7
            "question": "Is a GPA of 3.3 or higher required to apply for the Integrated Program?"
        },
        {
            # Q8 
            "question": "Do applicants need a recommendation from their undergraduate department head?"
        },
        {
            # Q9
            "question": "Are graduate courses recognized as undergraduate graduation credits?"
        },
        {
            # Q10
            "question": "Can students take up to 6 credits of graduate courses per semester before graduating?"
        }
    ]

    results_data = []
    start_time = time.time()

    for i, question in enumerate(questions, 1):
        q_start = time.time()  # ì§ˆë¬¸ ì‹œì‘ ì‹œê°„

        # ìƒìœ„ 3ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ê²€ìƒ‰
        results = vector_store.similarity_search(query=question["question"], k=1)
        combined_context = "\n".join([result.page_content for result in results])

        print(f"\n--- Example {i} ---")
        print(f"Question: {question['question']}")
        print(f"Context: {combined_context[:200]}...")  # context ìš”ì•½

        # ë‹µë³€ ìƒì„±
        answer = qa_generator.generate_answer(combined_context, question['question'])

        q_time = time.time() - q_start  # ì§ˆë¬¸ë³„ ì†Œìš” ì‹œê°„

        results_data.append({
            "question": question["question"],
            "context": combined_context,
            "answer": answer,
            "elapsed_time_sec": round(q_time, 3)
        })

        print(f"Answer: {answer}")
        print(f"â± Time for this question: {q_time:.2f} seconds")
        print("-" * 80)

    total_time = time.time() - start_time
    print(f"\nâ± Total Time Elapsed: {total_time:.2f} seconds")

    # DataFrame ì €ì¥
    df = pd.DataFrame(results_data)
    df.to_csv("qa_results.csv", index=False)
    print("ğŸ“„ Results saved to 'qa_results.csv'")




def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pdf_path", type=str, default="data/test_real.pdf")
    parser.add_argument("--chunk_method", type=str, choices=['character', 'recursive', 'semantic'], default='recursive')
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = get_args()
    main(args)




