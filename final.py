import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn.functional as F
from torch import nn
from transformers import GPT2Tokenizer, GPT2LMHeadModel

import argparse
from PyPDF2 import PdfReader
from langchain.schema import Document
from langchain_text_splitters  import CharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from bert_score import score

import pandas as pd
import time
import json

class QAGenerator(nn.Module):
    def __init__(self, model_name='gpt2-large'):
        super().__init__()
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits
    
    def get_device(self):
        return next(self.model.parameters()).device
    
    @torch.no_grad()
    def generate_answer(self, context, question, max_length=100, temperature=0.3, top_p=0.9):
        """ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± (Few-shot í”„ë¡¬í”„íŠ¸ í¬í•¨)"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""Please answer the question below using only the information provided in the context.
        If the context does not provide enough information, say "The document does not provide enough information."
        
        Context:
        {context}
        Question:
        {question}
        Answer:"""

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=900, truncation=True)
        inputs = inputs.to(self.get_device())
        
        # ìƒì„±
        for _ in range(max_length):
            # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
            logits = self.forward(inputs)
            next_token_logits = logits[0, -1, :] / temperature
            
            # Top-p ìƒ˜í”Œë§
            probs = F.softmax(next_token_logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
            
            # Top-p ë§ˆìŠ¤í¬ ìƒì„±
            mask = cumsum_probs <= top_p
            mask[1:] = mask[:-1].clone()
            mask[0] = True
            
            # í•„í„°ë§ëœ í™•ë¥ ë¡œ ìƒ˜í”Œë§
            filtered_probs = sorted_probs * mask.float()
            filtered_probs = filtered_probs / filtered_probs.sum()
            
            # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            next_token_idx = torch.multinomial(filtered_probs, 1)
            next_token = sorted_indices[next_token_idx]
            
            # ì¢…ë£Œ ì¡°ê±´ë“¤
            token_text = self.tokenizer.decode(next_token.item())
            if (next_token.item() == self.tokenizer.eos_token_id or 
                token_text in ['\n\nContext:', '\nContext:', 'Question:'] or
                '\n\n' in token_text):
                break
                
            # ìƒˆ í† í°ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
            inputs = torch.cat([inputs, next_token.unsqueeze(0)], dim=-1)
        
        # ë””ì½”ë”©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        generated_text = self.tokenizer.decode(inputs[0], skip_special_tokens=True)

        # print(f"\n\n\nGenerated text: {generated_text}\n\n\n")

        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆì§€ë§‰ Answer: ì´í›„ ë¶€ë¶„)
        if "Answer:" in generated_text:
            answer_parts = generated_text.split("Answer:")
            answer = answer_parts[-1].strip()
        else:
            answer = "Could not generate an answer."

        return answer


def pdf_to_text(pdf_path):
    """Convert PDF to text."""
    reader = PdfReader(pdf_path)
    pages = [page.extract_text() for page in reader.pages]
    return "\n".join(pages)


def chunk_text(text, chunk_method, embeddings_model=None, chunk_size=128):
    """Chunk text using the specified method."""
    if chunk_method == 'character':
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=chunk_size,
            chunk_overlap=100,
            length_function=len,
            is_separator_regex=False
        )
    elif chunk_method == 'recursive':
        text_splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", " ", ""],
            chunk_size=chunk_size,
            chunk_overlap=100,
            length_function=len,
            is_separator_regex=False
        )
    elif chunk_method == 'semantic':
        if embeddings_model:
            text_splitter = SemanticChunker(
                embeddings_model,
                breakpoint_threshold_type="percentile",
                breakpoint_threshold_amount=25  # ìƒìœ„ 25%ì—ì„œ ë¶„í• 
            )
        else:
            raise ValueError("Embeddings model must be provided for semantic chunking")
    else:
        raise ValueError("Unsupported chunk method")

    documents = [Document(page_content=text, metadata={"page_number": 1})]
    return text_splitter.split_documents(documents)


def save_to_file(data, file_path):
    """Save data to a file."""
    with open(file_path, 'w', encoding='utf-8') as f:
        if isinstance(data, (dict, list)):
            json.dump(data, f, ensure_ascii=False, indent=4)
        else:
            f.write(data)


def main(args):
    """ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ëª¨ë¸ ì´ˆê¸°í™”
    qa_generator = QAGenerator()
    qa_generator.to(device)
    qa_generator.eval()

    # PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # print(f"Reading PDF file: {args.pdf_path}")
    # text = pdf_to_text(args.pdf_path)
    # print("PDF text extraction complete.")
    # print(f"Extracted text length: {len(text)} characters")

    # # í…ìŠ¤íŠ¸ ì €ì¥
    # save_to_file(text, "extracted_text.txt")
    # print("ğŸ“„ Extracted text saved to 'extracted_text.txt'")

    # í…ìŠ¤íŠ¸ ì½ê¸°
    with open("extracted_text.txt", "r", encoding="utf-8") as f:
        text = f.read()

    # ì²­í‚¹ ì „ëµì„ ì¸ìë¡œ ë°›ì•„ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
    print(f"Chunking text using method: {args.chunk_method} and chunk size: {args.chunk_size}")
    documents = chunk_text(
        text,
        chunk_method=args.chunk_method,
        embeddings_model=OllamaEmbeddings(model="bge-m3") if args.chunk_method == 'semantic' else None,
        chunk_size=args.chunk_size
    )
    print(f"Number of chunks created: {len(documents)}")

    # ì²­í¬ ì €ì¥
    chunks_data = [doc.page_content for doc in documents]
    save_to_file(chunks_data, "chunks.json")
    print("ğŸ“„ Chunks saved to 'chunks.json'")

    # (4) ì²­í¬ë¥¼ ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ vectorí™”
    embeddings_model = OllamaEmbeddings(model="bge-m3")

    vector_store = FAISS.from_documents(
        documents=documents,
        embedding=embeddings_model
    )
    print("Vector store created with FAISS.")

    # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
    vector_store.save_local("vector_store")
    print("ğŸ“„ Vector store saved to 'vector_store' directory")

    # (5) ì§ˆë¬¸ ë°ì´í„°ì…‹ì„ ì½ì–´ì™€ì„œ vectorí™”ëœ ì²­í¬ì™€ ì§ˆë¬¸ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ì°¾ê¸°
    # (6) ê´€ë ¨ ì²­í¬(context)ì™€ ì§ˆë¬¸, few-shot ì˜ˆì œë¥¼ gpt2ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹µë³€ ìƒì„±

    questions = [
        {
            # Q1 â­ï¸
            "question": "By how much can the duration of study be reduced through the Integrated Program?"
            # article 2
        },
        {
            # Q2 â­ï¸ â­ï¸
            "question": "What is the intended benefit of the Undergraduate-Graduate Integrated Program regarding academic timeline?"
            # article 2
        },
        {
            # Q3
            "question": "What is the purpose of the Integrated Program in terms of study duration?"
            # article 2
        },
        {
            # Q4 â­ï¸ âŒ
            "question": "What documents are required to apply for the Integrated Program?"
            # article 4
        },
        {
            # Q5 â­ï¸
            "question": "How are applicants selected for the Integrated Program?"
            # article 5
        },
        {
            # Q6
            "question": "When are students eligible to apply for the Integrated Program based on their semester and earned credits?"
            # article 3.1
        },
        {
            # Q7
            "question": "What is the minimum GPA requirement to be eligible for the Integrated Program?"
            # article 3.2
        },
        {
            # Q8 
            "question": "Whose recommendations are required for applying to the Integrated Program?"
            # article 3.3 & 3.4
        },
        {
            # Q9
            "question": "Can graduate courses taken in the Integrated Program count toward undergraduate graduation requirements?"
            # article 7.2
        },
        {
            # Q10
            "question": "How many graduate credits can students take each semester before completing their undergraduate program?"
            # article 7.1
        }
    ]

    candidates = [] #BERTìš©
    results_data = []
    start_time = time.time()

    for i, question in enumerate(questions, 1):
        q_start = time.time()  # ì§ˆë¬¸ ì‹œì‘ ì‹œê°„

        # ìƒìœ„ 3ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ê²€ìƒ‰
        # results = vector_store.similarity_search(query=question["question"], k=5)

        # ê´€ë ¨ì„± ì ìˆ˜ë„ í•¨ê»˜ í™•ì¸
        results_with_scores = vector_store.similarity_search_with_score(query=question["question"], k=7)
        print(f"Retrieved {len(results_with_scores)} results for question {i}: {question['question']}")

        # ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì€ ê²ƒë“¤ í•„í„°ë§
        filtered_results = [doc for doc, score in results_with_scores if score < 0.7]  # ì„ê³„ê°’ ì¡°ì •

        combined_context = "\n".join([result.page_content for result in filtered_results[:3]])

        print(f"\n--- Example {i} ---")
        print(f"Question: {question['question']}")
        print(f"Context: {combined_context[:200]}...")  # context ìš”ì•½

        # ë‹µë³€ ìƒì„±
        answer = qa_generator.generate_answer(combined_context, question['question'])

        q_time = time.time() - q_start  # ì§ˆë¬¸ë³„ ì†Œìš” ì‹œê°„

        results_data.append({
            "question": question["question"],
            "context": combined_context,
            "answer": answer,
            "elapsed_time_sec": round(q_time, 3)
        })
        candidates.append(answer)

        print(f"Answer: {answer}")
        print(f"â± Time for this question: {q_time:.2f} seconds")
        print("-" * 80)
    

    # ì‹¤ì œ ë‹µë³€ê³¼ ì˜ˆì¸¡ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    references = [
        # Q1
        "The Integrated Program allows for the reduction of the duration of study by up to one year each for the bachelor's, master's, and integrated master's-doctoral degrees.",

        # Q2
        "The primary benefit of the Undergraduate-Graduate Integrated Program is that it enables students to complete their academic degrees in a shorter time by reducing the total duration of study by up to one year at each level.",

        # Q3
        "The purpose of the Integrated Program is to shorten the duration of study for bachelor's, master's, and integrated degrees by up to one year each.",

        # Q4
        "Applicants must submit an application form for the Integrated Program, a copy of their undergraduate academic transcript, a research plan, and a recommendation letter from the graduate advisor.",

        # Q5    
        "Applicants are selected through document screening, which considers undergraduate grades, research plans, recommendation letters, and other criteria set by the department.",

        # Q6
        "Students are eligible to apply for the Integrated Program after completing 4 to 7 semesters, provided they have earned at least 64, 80, 95, or 110 credits respectively, excluding seasonal session credits in the application semester.",

        # Q7
        "A cumulative GPA of 3.3 or higher is required up to the semester in which the student applies for the Integrated Program.",

        # Q8
        "Applicants need recommendations from both the head of their undergraduate department and the advisor of the intended graduate department. If applying to a different or interdisciplinary graduate department, an additional recommendation is required from the graduate department head.",

        # Q9
        "Graduate courses, except for shared undergraduate-graduate courses, are not recognized as undergraduate graduation credits. However, up to 12 credits of graduate courses can be completed before graduation, and some may be recognized as graduate credits later with approval.",

        # Q10
        "Students may take up to 6 credits of graduate courses per semester before undergraduate graduation, including shared undergraduate-graduate courses."
    ]

    # BERTScore ê³„ì‚°
    P, R, F1 = score(candidates, references, lang="en", verbose=True)

    # ê° ì ìˆ˜ ì¶œë ¥
    print(f"Precision: {P.mean():.4f}")
    print(f"Recall: {R.mean():.4f}")
    print(f"F1: {F1.mean():.4f}")
    total_time = time.time() - start_time
    print(f"\nâ± Total Time Elapsed: {total_time:.2f} seconds")

    # DataFrame ì €ì¥
    df = pd.DataFrame(results_data)
    df.to_csv("qa_results.csv", index=False)
    print("ğŸ“„ Results saved to 'qa_results.csv'")




def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pdf_path", type=str, default="data/test_real.pdf")
    parser.add_argument("--chunk_method", type=str, choices=['character', 'recursive', 'semantic'], default='recursive')
    parser.add_argument("--chunk_size", type=int, choices=[128, 256, 512, 1024], default=128) 
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = get_args()
    main(args)




